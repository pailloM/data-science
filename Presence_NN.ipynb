{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client, os, time\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature, set_signature\n",
    "import mlflow.keras\n",
    "import yaml\n",
    "from pickle import dump\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with open('Presences_keras.yaml', 'r') as file:\n",
    "    variables = yaml.safe_load(file)\n",
    "\n",
    "print(f\"{variables}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = influxdb_client.InfluxDBClient(\n",
    "    url=variables[\"influx_db\"][\"url\"],\n",
    "    token=variables[\"influx_db\"][\"token\"],\n",
    "    org=variables[\"influx_db\"][\"org\"],\n",
    "    verify_ssl=False,\n",
    "    timeout=180000)\n",
    "\n",
    "query_api = client.query_api()\n",
    "\n",
    "# |> range(start: 2024-02-20T17:00:00Z)\n",
    "start_time = \"2024-03-23T00:00:00Z\"\n",
    "query_start = datetime.strptime(start_time, \"%Y-%m-%dT%H:%M:%SZ\") - timedelta(days=5) \n",
    "\n",
    "base_query = \"\"\"\n",
    "        from(bucket: \"homeassistant\")\n",
    "            |> range(start: )\n",
    "            |> filter(fn: (r) => r[\"entity_id\"] == \"entity_name\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"value\")\n",
    "            |> fill(usePrevious: true)\n",
    "            |> drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"_field\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])\n",
    "            |> pivot(rowKey: [\"_time\"], columnKey: [\"entity_id\"], valueColumn: \"_value\")\n",
    "            |> yield(name: \"last\")\"\"\"\n",
    "base_query = base_query.replace(\"start: \", f'start: {datetime.strftime(query_start, \"%Y-%m-%dT%H:%M:%SZ\")}')\n",
    "for nb, entity in enumerate(variables[\"data\"]):\n",
    "    print(entity)\n",
    "    entity = entity.split(\":\")\n",
    "    query = base_query.replace(\"entity_name\", entity[0], 1)\n",
    "    if len(entity)> 1 and {entity[1]} != \"\":\n",
    "        query = query.replace('r[\"_field\"] == \"value\"', f'r[\"_field\"] == \"{entity[1]}\"')\n",
    "        query = query.replace('columnKey: [\"entity_id\"]', f'columnKey: [\"_field\"]')\n",
    "        query = query.replace(\n",
    "            'drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"_field\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])',\n",
    "            'drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"entity_id\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])'\n",
    "        )\n",
    "    if len(entity)> 2 and {entity[2]} != \"\":\n",
    "       query = query.replace('fn: last,', f'fn: {entity[2]},')\n",
    "    # print(query)\n",
    "    df = query_api.query_data_frame(query, org=variables[\"influx_db\"][\"org\"])\n",
    "    print(df.head())\n",
    "    try:\n",
    "        df.set_index('_time', inplace=True)\n",
    "        df.drop([\"result\", \"table\"], axis=1, inplace=True)\n",
    "        if nb == 0:\n",
    "            full_df = df.copy()\n",
    "        else:\n",
    "            full_df = full_df.join(df,on=\"_time\", how='outer')\n",
    "    except KeyError:\n",
    "        print(f\"{entity[1]} was not found\")\n",
    "        if len(entity)> 2:\n",
    "            full_df[entity[1]] =  entity[2]\n",
    "        else: \n",
    "            full_df[entity[1]] = np.nan\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.shape)\n",
    "print(full_df.head())\n",
    "feature_names = variables[\"features\"]\n",
    "\n",
    "for feature in variables[\"numeric_features\"]:\n",
    "    full_df[feature] = pd.Series.interpolate(full_df[feature])\n",
    "\n",
    "full_df.ffill(inplace=True)\n",
    "full_df[\"home_status\"] = full_df[\"in_bed\"] + full_df[\"presence\"]\n",
    "full_df = full_df[full_df['ha_started']==1]\n",
    "full_df.reset_index(inplace=True)\n",
    "full_df = full_df[full_df['_time']> start_time]\n",
    "new_df = full_df[variables[\"features\"] + variables[\"targets\"]].copy()\n",
    "print(new_df.head())\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.reset_index(inplace=True)\n",
    "new_df.drop_duplicates(feature_names, inplace=True, ignore_index=True)\n",
    "print(new_df.dtypes)\n",
    "\n",
    "\n",
    "print(new_df.shape)\n",
    "\n",
    "target = new_df[variables[\"targets\"]]\n",
    "numeric_features = new_df[feature_names]\n",
    "numeric_features.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"http://192.168.0.2:5051\"\n",
    "mlflow.set_tracking_uri(uri=\"http://192.168.0.2:5051\")\n",
    "mlflow.set_tracking_uri(uri=variables[\"mlflow\"][\"url\"])\n",
    "mlflow.set_experiment(\"Presence detection generalization keras.\")\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, MaxPooling1D, Normalization, Dropout, Reshape, Conv2D, MaxPooling2D, Input\n",
    "from tensorflow.keras.metrics import F1Score, Recall, Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    numeric_features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, pipeline setup and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (X_train.values.shape[1],) \n",
    "\n",
    "mean = np.mean(X_train.values)\n",
    "variance = np.var(X_train.values)\n",
    "print(f\"mean: {mean}\")\n",
    "print(f\"variance: {variance}\")\n",
    "\n",
    "mlflow.set_tag(\"type\", \"CNN\", synchronous=False)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(Normalization(mean=mean, variance=variance, axis=None))\n",
    "model.add(Reshape((3,6)))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "mlflow.set_tag(\"type\", \"RNN\", synchronous=False)\n",
    "# model.add(Reshape((3,3,2)))\n",
    "# model.add(Conv2D(256, (1,1), activation='relu'))\n",
    "# model.add(MaxPooling2D(1,1))\n",
    "# model.add(Conv2D(256, (1,1), activation='relu'))\n",
    "# model.add(MaxPooling2D(3,3))\n",
    "# mlflow.set_tag(\"type\", \"CNN\", synchronous=False)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9, activation='sigmoid'))\n",
    "model.summary()\n",
    "# model = Sequential([\n",
    "#     Input(shape=input_shape),\n",
    "#     Normalization(mean=mean, variance=variance, axis=None),\n",
    "# ##############################    \n",
    "#    Reshape((3,3,2)),\n",
    "#    Conv2D(256, (1,1), activation='relu'),\n",
    "#    MaxPooling2D(1,1),\n",
    "#    Conv2D(256, (1,1), activation='relu'),\n",
    "#    MaxPooling2D(3,3),\n",
    "# #    mlflow.set_tag(\"type\", \"CNN\", synchronous=False)\n",
    "# ############################\n",
    "#     # Reshape((3,6)),\n",
    "#     # LSTM(512, input_shape=input_shape,  return_sequences=True),\n",
    "# #    mlflow.set_tag(\"type\", \"RNN\", synchronous=False)\n",
    "# ############################\n",
    "#     Flatten(),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(9, kernel_initializer='normal', activation='sigmoid')  # 2 output targets\n",
    "# ])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'binary_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 25\n",
    "epochs = 125\n",
    "\n",
    "history = model.fit(X_train.values, y_train.values, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "run_dict =  mlflow.last_active_run().to_dictionary() \n",
    "print(run_dict)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dict =  mlflow.last_active_run().to_dictionary() \n",
    "print(run_dict)\n",
    "signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "set_signature(f\"runs:/{run_dict['info']['run_id']}/model\", signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay, jaccard_score\n",
    "metrics_dict = {}\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "print(y_pred[0])\n",
    "for idx, array in enumerate(y_pred):\n",
    "    # print(y_pred[idx].shape)\n",
    "    y_pred[idx] = (y_pred[idx]>0.7).astype(int)\n",
    "\n",
    "print(y_pred[0])\n",
    "\n",
    "print(y_pred.shape)\n",
    "\n",
    "metrics_dict[\"jaccard score macro\"] = jaccard_score(y_test.values, y_pred, average='macro')\n",
    "print(metrics_dict[\"jaccard score macro\"] )\n",
    "\n",
    "jaccard_score_array = jaccard_score(y_test.values, y_pred, average=None)\n",
    "print(jaccard_score_array)\n",
    "cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "cm_dict = {}\n",
    "jaccard_score_dict = {}\n",
    "for idx, target in enumerate(variables[\"targets\"]):\n",
    "    cm_dict[target] = cm[idx]\n",
    "    jaccard_score_dict[target] = jaccard_score_array[idx]\n",
    "    cm_disp = ConfusionMatrixDisplay(cm_dict[target])\n",
    "    cm_disp.plot(cmap=plt.cm.Blues,)\n",
    "    plt.title(f\"{target}\")\n",
    "    plt.savefig(f\"test_confusion_matrix_{target}.png\")\n",
    "    shutil.copy(f\"test_confusion_matrix_{target}.png\", f\"/mnt/nfs/mlflow/{run_dict['info']['experiment_id']}/{run_dict['info']['run_id']}/artifacts/\")\n",
    "\n",
    "print(cm)\n",
    "mlflow.log_metrics(metrics_dict, run_id=f\"{run_dict['info']['run_id']}\")\n",
    "mlflow.log_table(cm_dict, artifact_file=\"confusion_matrix\", run_id=f\"{run_dict['info']['run_id']}\")\n",
    "mlflow.log_table(jaccard_score_dict, artifact_file=\"jaccard_score\", run_id=f\"{run_dict['info']['run_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run() as run:\n",
    "#     mlflow.autolog()\n",
    "#     import autokeras as ak\n",
    "#     os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "#     mlflow.set_tracking_uri(uri=variables[\"mlflow\"][\"url\"])\n",
    "#     mlflow.set_experiment(\"Presence detection generalization keras.\")\n",
    "#     # define the search\n",
    "#     search = ak.AutoModel(\n",
    "#         inputs=[ak.Input()],\n",
    "#         outputs=[\n",
    "#             ak.ClassificationHead(\n",
    "#                 loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "#             ),\n",
    "#         ],\n",
    "#     overwrite=True,\n",
    "#     max_trials=100,\n",
    "#     )\n",
    "#     # perform the search\n",
    "#     history = search.fit(x=numeric_features.values, y=target.values, validation_split=0.2,)\n",
    "\n",
    "#     accuracy, _ = search.evaluate(numeric_features.values, target.values, verbose=0)\n",
    "#     print('Accuracy: %.3f' % accuracy)\n",
    "\n",
    "#     # get the best performing model\n",
    "#     model = search.export_model()\n",
    "#     signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "#     # summarize the loaded model\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dict =  mlflow.last_active_run().to_dictionary() \n",
    "# print(run_dict)\n",
    "# signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "# set_signature(f\"runs:/{run_dict['info']['run_id']}/model\", signature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
