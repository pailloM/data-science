{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client, os, time\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature, set_signature\n",
    "import mlflow.keras\n",
    "import yaml\n",
    "from pickle import dump\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with open('thermostat.yaml', 'r') as file:\n",
    "    variables = yaml.safe_load(file)\n",
    "\n",
    "print(f\"{variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = influxdb_client.InfluxDBClient(\n",
    "    url=variables[\"influx_db\"][\"url\"],\n",
    "    token=variables[\"influx_db\"][\"token\"],\n",
    "    org=variables[\"influx_db\"][\"org\"],\n",
    "    verify_ssl=False,\n",
    "    timeout=3600000\n",
    ")\n",
    "\n",
    "query_api = client.query_api()\n",
    "\n",
    "# range = start: 2022-01-01T00:00:00Z, stop: 2023-08-21T09:30:30Z)\n",
    "start_time = \"2022-01-01T00:00:00Z\"\n",
    "query_start = datetime.strptime(start_time, \"%Y-%m-%dT%H:%M:%SZ\") - timedelta(days=5) \n",
    "\n",
    "base_query = \"\"\"\n",
    "        from(bucket: \"homeassistant\")\n",
    "            |> range(start: )\n",
    "            |> filter(fn: (r) => r[\"entity_id\"] == \"entity_name\")\n",
    "            |> filter(fn: (r) => r[\"_field\"] == \"value\")\n",
    "            |> fill(usePrevious: true)\n",
    "            |> drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"_field\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])\n",
    "            |> pivot(rowKey: [\"_time\"], columnKey: [\"entity_id\"], valueColumn: \"_value\")\n",
    "            |> yield(name: \"last\")\"\"\"\n",
    "base_query = base_query.replace(\"start: \", f'start: {datetime.strftime(query_start, \"%Y-%m-%dT%H:%M:%SZ\")}')\n",
    "for nb, entity in enumerate(variables[\"data\"]):\n",
    "    print(entity)\n",
    "    entity = entity.split(\":\")\n",
    "    query = base_query.replace(\"entity_name\", entity[0], 1)\n",
    "    if len(entity)> 1 and {entity[1]} != \"\":\n",
    "        query = query.replace('r[\"_field\"] == \"value\"', f'r[\"_field\"] == \"{entity[1]}\"')\n",
    "        query = query.replace('columnKey: [\"entity_id\"]', f'columnKey: [\"_field\"]')\n",
    "        query = query.replace(\n",
    "            'drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"_field\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])',\n",
    "            'drop(columns: [\"result\", \"table\", \"_start\", \"_stop\", \"entity_id\", \"source\",\"domain\",\"_measurement\", \"friendly_name\"])'\n",
    "        )\n",
    "    if len(entity)> 2 and {entity[2]} != \"\":\n",
    "       query = query.replace('fn: last,', f'fn: {entity[2]},')\n",
    "    # print(query)\n",
    "    df = query_api.query_data_frame(query, org=variables[\"influx_db\"][\"org\"])\n",
    "    print(df.head())\n",
    "    try:\n",
    "        df.set_index('_time', inplace=True)\n",
    "        df.drop([\"result\", \"table\"], axis=1, inplace=True)\n",
    "        if nb == 0:\n",
    "            full_df = df.copy()\n",
    "        else:\n",
    "            full_df = full_df.join(df,on=\"_time\", how='outer')\n",
    "    except KeyError:\n",
    "        print(f\"{entity[1]} was not found\")\n",
    "        if len(entity)> 2:\n",
    "            full_df[entity[1]] =  entity[2]\n",
    "        else: \n",
    "            full_df[entity[1]] = np.nan\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.head(10))\n",
    "feature_names = variables[\"features\"]\n",
    "\n",
    "for feature in variables[\"numeric_features\"]:\n",
    "    full_df[feature] = pd.Series.interpolate(full_df[feature])\n",
    "\n",
    "full_df.ffill(inplace=True)\n",
    "full_df[\"home_status\"] = full_df[\"in_bed\"] + full_df[\"presence\"]\n",
    "\n",
    "print(full_df.shape)\n",
    "full_df = full_df[full_df['ha_started']==1]\n",
    "mask_heat = full_df['state']=='heat'\n",
    "mask_cool = full_df['state']=='cool'\n",
    "mask_off = full_df['state']=='off'\n",
    "full_df.loc[mask_heat, 'target_temp_low'] = full_df.loc[mask_heat, 'temperature']\n",
    "full_df.loc[mask_cool, 'target_temp_high'] = full_df.loc[mask_cool, 'temperature']\n",
    "full_df.loc[mask_off, 'target_temp_low'] = 14\n",
    "full_df.loc[mask_off, 'target_temp_high'] = 34\n",
    "full_df.loc[pd.to_numeric(full_df['target_temp_low']) < 14, 'target_temp_low'] = 14\n",
    "full_df.loc[pd.to_numeric(full_df['target_temp_low']) > 21, 'target_temp_low'] = 20.5\n",
    "full_df.loc[pd.to_numeric(full_df['target_temp_high']) > 34, 'target_temp_high'] = 34\n",
    "full_df.loc[pd.to_numeric(full_df['target_temp_high']) < 20, 'target_temp_high'] = 34\n",
    "full_df['target_temp_low'].fillna(14)\n",
    "full_df['target_temp_high'].fillna(34)\n",
    "full_df.reset_index(inplace=True)\n",
    "new_df = full_df[['_time'] + variables[\"features\"] + variables[\"targets\"]].copy()\n",
    "new_df.reset_index(inplace=True)\n",
    "new_df.dropna(inplace=True)\n",
    "new_df.drop_duplicates(feature_names, inplace=True, ignore_index=True)\n",
    "print(new_df.head(10))\n",
    "new_df = new_df[new_df['_time']> start_time].copy()\n",
    "print(new_df.shape)\n",
    "\n",
    "target = new_df[variables[\"targets\"]].copy()\n",
    "numeric_features = new_df[feature_names].copy()\n",
    "numeric_features.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.head(10))\n",
    "print(full_df.head(10))\n",
    "\n",
    "#target.loc[target[\"target_temp_low\"]>25, \"target_temp_low\"] = 20.5\n",
    "plt.subplot(311)\n",
    "plt.plot(new_df['_time'], target[\"target_temp_high\"], )\n",
    "plt.subplot(312)\n",
    "plt.plot(new_df['_time'], target[\"target_temp_low\"])\n",
    "plt.subplot(313)\n",
    "plt.plot(full_df['_time'], full_df[\"temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "mlflow.set_tracking_uri(uri=variables[\"mlflow\"][\"url\"])\n",
    "mlflow.set_experiment(\"Thermostat setpoint\")\n",
    "\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, MaxPooling1D, Normalization, Dropout, Reshape, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.metrics import R2Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, pipeline setup and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (numeric_features.values.shape[1],) \n",
    "\n",
    "mean = np.mean(numeric_features.values)\n",
    "variance = np.var(numeric_features.values)\n",
    "print(f\"mean: {mean}\")\n",
    "print(f\"variance: {variance}\")\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    #Conv1D(32, 8, input_shape=input_shape, activation='relu'),\n",
    "    # MaxPooling1D(2,2),\n",
    "    Normalization( input_shape=input_shape, mean=mean, variance=variance, axis=None),\n",
    "    Reshape((3,6)),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    # Conv2D(128, (2,3), activation='relu'),\n",
    "    # MaxPooling2D(1,2),\n",
    "    # Conv1D(256, 1, activation='relu'),\n",
    "    # MaxPooling1D(1,2),\n",
    "    Flatten(),\n",
    "    Dense(512, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, kernel_initializer='normal', activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, kernel_initializer='normal')  # 2 output targets\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=[R2Score(), 'mse', 'mae', 'mape'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 25\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(numeric_features.values, target.values, epochs=epochs, validation_split=0.2)\n",
    "\n",
    "run_dict =  mlflow.last_active_run().to_dictionary() \n",
    "print(run_dict)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "set_signature(f\"runs:/{run_dict['info']['run_id']}/model\", signature)\n",
    "\n",
    "print(numeric_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with mlflow.start_run() as run:\n",
    "#     mlflow.autolog()\n",
    "#     import autokeras as ak\n",
    "#     os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "#     mlflow.set_tracking_uri(uri=variables[\"mlflow\"][\"url\"])\n",
    "#     mlflow.set_experiment(\"Thermostat setpoint\")\n",
    "#     signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "#     # define the search\n",
    "#     search = ak.StructuredDataRegressor(max_trials=100, loss='mean_absolute_error',  metrics=['mse', 'mae', 'mape'])\n",
    "#     # perform the search\n",
    "#     history = search.fit(x=numeric_features.values, y=target.values, validation_split=0.2,)\n",
    "\n",
    "#     mae, mse, mape, _ = search.evaluate(numeric_features.values, target.values, verbose=0)\n",
    "#     print('MAE: %.3f' % mae)\n",
    "\n",
    "#     # get the best performing model\n",
    "#     model = search.export_model()\n",
    "#     # summarize the loaded model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dict =  mlflow.last_active_run().to_dictionary() \n",
    "# print(run_dict)\n",
    "# signature = infer_signature(numeric_features, model.predict(numeric_features))\n",
    "# set_signature(f\"runs:/{run_dict['info']['run_id']}/model\", signature)\n",
    "\n",
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['mse'])\n",
    "# # plt.plot(history.history['val_mse'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('mse')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# # plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_df.to_csv('thermostat_2022-23.csv', encoding='utf-8')\n",
    "#!export MLFLOW_TRACKING_URI=variables[\"mlflow\"][\"url\"]\n",
    "#!mlflow models build-docker -m runs:/57e811d9acd34c0db02d762d6fa6c31b/model -n paillomams/presence-det --enable-mlserver\n",
    "\n",
    "#!docker push paillomams/presence-det"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
